# 외부 데이터를 활용한 데이터 수집

TMDB에서 요청하여 가져올 예정

## 웹 스크래핑 (웹 크롤링) with Python

query 뒤의 값 바꿔서 받아오는 데이터 변경 가능 (구글은 q)

요청을 통해 문서를 받아올 수 있음

요청 --> 클라이언트 (나)

응답 --> 서버 (naver)

**파이썬** 을 통해 주소로 요청 보내고 거기서 수정!

## 요청과 응답에 대한 이해

requests 라이브러리 설치!

```python
# 웹 사이트의 정보를 가지고 오고싶다.
import requests
# 1. 주소
URL = 'https://finance.naver.com/sise/'
# 2. 요청
response = requests.get(URL)
# response (200) 이 나오면 성공적으로 데이터 가져온 것
print(response)
```

BeautifulSoup 설치!

HTML , XML 파일에서 파일 타입 다르게 만들고 원하는 데이터 가져올 수 있음

```python
# 웹 사이트의 정보를 가지고 오고싶다.
import requests
from bs4 import BeautifulSoup
# 1. 주소
URL = 'https://finance.naver.com/sise/'
# 2. 요청
response = requests.get(URL)
# response (200) 이 나오면 성공적으로 데이터 가져온 것
print(response)

# BeautifulSoup (text --> 다른 객체)
data = BeautifulSoup(response, 'html.pareser')

# 내장 모듈 사용 (select_one : 정보 하나 가져와서 뽑음)
kospi = data.select_one('selector 경로: 크롬 분석에서 찾아오기')

```



## API의 활용과 문서 분석

API (Application Programming Interface)

주소, 문서의 형태를 확인해야 한다

### 요청

인증 방식

URL 생성

### 응답

응답 결과 타입, 응답 결과 구조

```python
# agify 실습

import requests

# 1. uRL 
# 요청 변수 처리: ? 뒤에 있는 것
URL = 'https://api.agify.io?name=michael'

# 2. 요청
# 얘는 json 파일이므로 BeuatifulSoup 사용 안 해도 됨
response =  requests.get(URL).json()
print(response)

# 요청 변수를 따로 dictionary 형태로 설정하고 뒤에 params = dictionary 이름 해도 된다
URL = 'https://api.agify.io'
parameter = {'name': 'michael'}

response =  requests.get(URL, params = parameter).json()
print(response)
```

```python
# TMDB 실습

# 0. import
import requests

# 1. URL 및 요청변수 설정
BASE_URL = 'https://api.themoviedb.org/3'
path = '/movie/now_playing'
parameter = {
    'api_key' = '내 key값'
    'region' = 'KR'
    'language' = 'KO'
}

# 2. 요청 보낸 결과 저장
response = requests.get(BASE_URL + path, params = parameter).json()

# 3. 조작
print(response)
```

